{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9546c16-758a-4f11-9622-636e65f0a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from scipy.linalg import solve\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e4607f2-65bb-47c0-a4c8-b6bacc0efab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ev=15\n",
    "n_iter=10\n",
    "norm='L2'\n",
    "\n",
    "spectra = np.load(...)\n",
    "mask = np.load(...)\n",
    "\n",
    "mask_num = np.sum(mask,axis=1)\n",
    "\n",
    "spectra = spectra[mask_num<300]\n",
    "mask = mask[mask_num<300]\n",
    "Table = Table[mask_num<300]\n",
    "\n",
    "spectra[np.isnan(spectra)]=0\n",
    "\n",
    "X = np.asarray(spectra, dtype=float)\n",
    "M = np.asarray(mask, dtype=bool)\n",
    "\n",
    "if X.shape != M.shape:\n",
    "    raise ValueError('X and M must have the same shape')\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "if np.any(M.sum(0) == n_samples):\n",
    "    raise ValueError('Some features are masked in all samples')\n",
    "\n",
    "if type(norm) == str:\n",
    "    norm = norm.upper()\n",
    "\n",
    "if norm not in (None, 'none', 'L1', 'L2'):\n",
    "    raise ValueError('unrecognized norm: %s' % norm)\n",
    "\n",
    "notM = (~M)\n",
    "X_recons = X.copy()\n",
    "\n",
    "X_recons[M] = 0\n",
    "\n",
    "# as an initial guess, we'll fill-in masked regions with the mean\n",
    "# of the rest of the sample\n",
    "\n",
    "if norm is None:\n",
    "    mu = (X_recons * (notM)).sum(0) / (notM).sum(0)\n",
    "    mu = mu * np.ones([n_samples, 1])\n",
    "    X_recons[M] = mu[M]\n",
    "else:\n",
    "    # since we're normalizing each spectrum, and the norm depends on\n",
    "    # the filled-in values, we need to iterate a few times to make\n",
    "    # sure things are consistent.\n",
    "    for i in range(n_iter):\n",
    "        print('norm',i)\n",
    "        # normalize\n",
    "        if norm == 'L1':\n",
    "            X_recons /= np.sum(X_recons, 1)[:, None]\n",
    "        else:\n",
    "            X_recons /= np.sqrt(np.sum(X_recons ** 2, 1))[:, None]\n",
    "\n",
    "        # find the mean\n",
    "        mu = (X_recons * (notM)).sum(0) / (notM).sum(0)\n",
    "        mu = mu * np.ones([n_samples, 1])\n",
    "        X_recons[M] = mu[M]\n",
    "\n",
    "# Matrix of coefficients\n",
    "coeffs = np.zeros((n_samples, n_ev))\n",
    "\n",
    "# Now we iterate through, using the principal components to reconstruct\n",
    "#  these regions.\n",
    "\n",
    "print('n_iter')\n",
    "\n",
    "for i in range(n_iter):\n",
    "    sys.stdout.write(' PCA iteration %i / %i\\r' % (i + 1, n_iter))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # normalize the data\n",
    "    if norm == 'L1':\n",
    "        X_recons /= np.sum(X_recons, 1)[:, None]\n",
    "    else:\n",
    "        X_recons /= np.sqrt(np.sum(X_recons ** 2, 1))[:, None]\n",
    "\n",
    "    # now compute the principal components\n",
    "    mu = X_recons.mean(0)\n",
    "    X_centered = X_recons - mu\n",
    "\n",
    "    U, S, VT = np.linalg.svd(X_centered, full_matrices=False)\n",
    "\n",
    "    # perform a least-squares fit to estimate the coefficients of the\n",
    "    # first n_ev eigenvectors for each data point.\n",
    "    # The eigenvectors are in the rows of the matrix VT.\n",
    "    # The coefficients are given by\n",
    "    #  a_n = [V_n^T W V_n]^(-1) V_n W x\n",
    "    # Such that x can be reconstructed via\n",
    "    #  x_n = V_n a_n\n",
    "    # Variables here are:\n",
    "    #  x   : vector length n_features. This is a data point to be\n",
    "    #        reconstructed\n",
    "    #  a_n : vector of length n.  These are the reconstruction weights\n",
    "    #  V_n : eigenvector matrix of size (n_features, n).\n",
    "    #  W   : diagonal weight matrix of size (n_features, n_features)\n",
    "    #        such that W[i,i] = M[i]\n",
    "    #  x_n : vector of length n_features which approximates x\n",
    "    VWx = np.dot(VT[:n_ev], ((notM) * X_centered).T)\n",
    "    for i in range(n_samples):\n",
    "        VWV = np.dot(VT[:n_ev], ((notM)[i] * VT[:n_ev]).T)\n",
    "        coeffs[i] = solve(VWV, VWx[:, i], assume_a=\"pos\", overwrite_a=True)\n",
    "\n",
    "    X_fill = mu + np.dot(coeffs, VT[:n_ev])\n",
    "    X_recons[M] = X_fill[M]\n",
    "sys.stdout.write('\\n')\n",
    "\n",
    "# un-normalize X_recons\n",
    "norms = np.zeros(n_samples)\n",
    "for i in range(n_samples):\n",
    "    ratio_i = X[i][(notM)[i]] / X_recons[i][(notM)[i]]\n",
    "    norms[i] = ratio_i[~np.isnan(ratio_i)][0]\n",
    "    X_recons[i] *= norms[i]\n",
    "np.save('path_to_save',X_recons)\n",
    "\n",
    "# We normalized this data to set their median at 0.5 before we trained VAE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2_env",
   "language": "python",
   "name": "torch2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
